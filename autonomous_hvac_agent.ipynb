{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m6aONku6hEl",
        "outputId": "01b04c4a-e699-4e32-e852-cd23543a5788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai faiss-cpu pandas tiktoken langchain-community --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from getpass import getpass\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "\n",
        "from getpass import getpass\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0.3)\n",
        "\n",
        "\n",
        "RULES_PATH = \"rules.json\"\n",
        "if not os.path.exists(RULES_PATH):\n",
        "    with open(RULES_PATH, 'w') as f:\n",
        "        json.dump({\"air_flow\": 0.5, \"temperature\": 30.0}, f)\n",
        "with open(RULES_PATH) as f:\n",
        "    rules = json.load(f)\n",
        "\n",
        "def rule_based_diagnosis(data):\n",
        "    r = rules\n",
        "    if data[\"air_flow\"] < r[\"air_flow\"] and not data[\"compressor_status\"]:\n",
        "        return \"Fault\", \"Low airflow + compressor OFF\", \"Low\"\n",
        "    if data[\"temperature\"] > r[\"temperature\"] and not data[\"fan_status\"]:\n",
        "        return \"Fault\", \"High temp + fan OFF\", \"Medium\"\n",
        "    return \"No Fault\", \"Normal\", \"High\"\n",
        "\n",
        "\n",
        "if not os.path.exists(\"faiss_memory\"):\n",
        "    db = FAISS.from_texts([\"seed\"], OpenAIEmbeddings())\n",
        "    db.save_local(\"faiss_memory\")\n",
        "db = FAISS.load_local(\"faiss_memory\", OpenAIEmbeddings())\n",
        "memory = VectorStoreRetrieverMemory(retriever=db.as_retriever())\n",
        "\n",
        "\n",
        "def actuator_control(action):\n",
        "    return f\"[Actuator] {action} simulated.\"\n",
        "\n",
        "def update_rule(json_str):\n",
        "    update = json.loads(json_str)\n",
        "    rules.update(update)\n",
        "    with open(RULES_PATH, 'w') as f:\n",
        "        json.dump(rules, f)\n",
        "    return f\"Rule updated: {update}\"\n",
        "\n",
        "def store_case(input_data):\n",
        "    db.add_texts([json.dumps(input_data)])\n",
        "    return \"Case stored.\"\n",
        "\n",
        "tools = [\n",
        "    Tool(\"ActuatorControl\", actuator_control, \"Simulate control actions\"),\n",
        "    Tool(\"RuleUpdater\", update_rule, \"Update the rulebook live\"),\n",
        "    Tool(\"MemoryStore\", store_case, \"Store input in FAISS memory\")\n",
        "]\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, memory=memory)\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "You're an autonomous HVAC agent.\n",
        "Input:\n",
        "{sensor_data}\n",
        "Return JSON: [\"Diagnosis\", \"Reason\", \"Confidence\", \"Recommended Action\", \"Rule Suggestion\"]\n",
        "\"\"\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "logfile = \"diagnostic_log.csv\"\n",
        "if not os.path.exists(logfile):\n",
        "    with open(logfile, 'w') as f:\n",
        "        f.write(\"timestamp,rule_diagnosis,gpt_diagnosis,ground_truth,correct,gpt_confidence,action,rule_suggestion\\n\")\n",
        "\n",
        "def run_agent_on_row(row, gt):\n",
        "    input_data = {\n",
        "        \"temperature\": float(row.get(\"Zone Temperature\", 25.0)),\n",
        "        \"humidity\": float(row.get(\"Zone Humidity\", 50.0)),\n",
        "        \"air_flow\": float(row.get(\"Supply Air Flow Rate\", 1.0)),\n",
        "        \"fan_status\": row.get(\"Fan Status\", 1.0) > 0,\n",
        "        \"compressor_status\": row.get(\"Compressor Status\", 1.0) > 0\n",
        "    }\n",
        "    rule_diag, _, _ = rule_based_diagnosis(input_data)\n",
        "    result = chain.run(sensor_data=json.dumps(input_data))\n",
        "    try:\n",
        "        parsed = json.loads(result)\n",
        "    except:\n",
        "        parsed = {\"Diagnosis\": \"Parse Error\", \"Confidence\": \"Low\", \"Recommended Action\": \"None\", \"Rule Suggestion\": \"{}\"}\n",
        "\n",
        "    if parsed[\"Recommended Action\"] != \"None\":\n",
        "        agent.run(f\"ActuatorControl: {parsed['Recommended Action']}\")\n",
        "    if parsed[\"Rule Suggestion\"] != \"{}\":\n",
        "        agent.run(f\"RuleUpdater: {parsed['Rule Suggestion']}\")\n",
        "    agent.run(f\"MemoryStore: {json.dumps(input_data)}\")\n",
        "\n",
        "    correct = parsed[\"Diagnosis\"] == gt\n",
        "    with open(logfile, 'a') as f:\n",
        "        f.write(f\"{datetime.now()},{rule_diag},{parsed['Diagnosis']},{gt},{correct},{parsed['Confidence']},{parsed['Recommended Action']},{parsed['Rule Suggestion']}\\n\")\n",
        "\n",
        "\n",
        "def run_on_csv(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if \"Fault Detection Ground Truth\" not in df.columns:\n",
        "        print(\"Missing ground truth column.\")\n",
        "        return\n",
        "    df = df.dropna(subset=[\"Fault Detection Ground Truth\"])\n",
        "    df = df.sample(n=min(25, len(df)))\n",
        "    for _, row in df.iterrows():\n",
        "        label = \"Fault\" if row[\"Fault Detection Ground Truth\"] == 1 else \"No Fault\"\n",
        "        run_agent_on_row(row, label)\n",
        "    print(\"Completed run. Logs saved.\")"
      ],
      "metadata": {
        "id": "VLmdoO03xBdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}